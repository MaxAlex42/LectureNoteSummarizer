%%
% Copyright (c) 2017 - 2025, Pascal Wagler;
% Copyright (c) 2014 - 2025, John MacFarlane
%
% All rights reserved.
%
% Redistribution and use in source and binary forms, with or without
% modification, are permitted provided that the following conditions
% are met:
%
% - Redistributions of source code must retain the above copyright
% notice, this list of conditions and the following disclaimer.
%
% - Redistributions in binary form must reproduce the above copyright
% notice, this list of conditions and the following disclaimer in the
% documentation and/or other materials provided with the distribution.
%
% - Neither the name of John MacFarlane nor the names of other
% contributors may be used to endorse or promote products derived
% from this software without specific prior written permission.
%
% THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
% "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
% LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
% FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
% COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
% INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
% BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
% LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
% CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
% LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
% ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
% POSSIBILITY OF SUCH DAMAGE.
%%

%%
% This is the Eisvogel pandoc LaTeX template.
%
% For usage information and examples visit the official GitHub page:
% https://github.com/Wandmalfarbe/pandoc-latex-template
%%
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names,table}{xcolor}
\documentclass[
12pt,
paper=a4,
,captions=tableheading
]{scrartcl}
\usepackage{xcolor}
\usepackage[margin=2.5cm,includehead=true,includefoot=true,centering,]{geometry}
\usepackage{amsmath,amssymb}

% add backlinks to footnote references, cf. https://tex.stackexchange.com/questions/302266/make-footnote-clickable-both-ways
\usepackage{footnotebackref}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
\usepackage{unicode-math} % this also loads fontspec
\defaultfontfeatures{Scale=MatchLowercase}
\defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
% xetex/luatex font selection
\setmainfont[]{Times New Roman}
\setmonofont[]{Courier}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
	\usepackage[]{microtype}
	\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}

% Use setspace anyway because we change the default line spacing.
% The spacing is changed early to affect the titlepage and the TOC.
\usepackage{setspace}
\setstretch{1.2}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
	\IfFileExists{parskip.sty}{%
		\usepackage{parskip}
	}{% else
		\setlength{\parindent}{0pt}
		\setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
	\KOMAoptions{parskip=half}}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
	\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
% number down to subsections
\setcounter{secnumdepth}{2}
% show subsections as “1, 2, 3…” instead of “0.1, 0.2…”
\renewcommand\thesubsection{\arabic{subsection}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\definecolor{default-linkcolor}{HTML}{A50000}
\definecolor{default-filecolor}{HTML}{A50000}
\definecolor{default-citecolor}{HTML}{4077C0}
\definecolor{default-urlcolor}{HTML}{4077C0}

\hypersetup{
	pdftitle={Problem Description},
	pdfauthor={auth1 auth2},
	hidelinks,
	breaklinks=true,
	pdfcreator={LaTeX via pandoc with the Eisvogel template}}

\title{Problem Description\\[1ex]\large[510.607]~Neuere Methoden in der Computerlinguistik}
\author{Maximilian Sellner Hannah Tomasini}
\date{\today}

%
% for the background color of the title page
%

%
% break urls
%
\PassOptionsToPackage{hyphens}{url}

%
% When using babel or polyglossia with biblatex, loading csquotes is recommended
% to ensure that quoted texts are typeset according to the rules of your main language.
%
\usepackage{csquotes}

%
% captions
%
\definecolor{caption-color}{HTML}{777777}
\usepackage[font={stretch=1.2}, textfont={color=caption-color}, position=top, skip=4mm, labelfont=bf, singlelinecheck=false, justification=raggedright]{caption}
\setcapindent{0em}

%
% blockquote
%
\definecolor{blockquote-border}{RGB}{221,221,221}
\definecolor{blockquote-text}{RGB}{119,119,119}
\usepackage{mdframed}
\newmdenv[rightline=false,bottomline=false,topline=false,linewidth=3pt,linecolor=blockquote-border,skipabove=\parskip]{customblockquote}
\renewenvironment{quote}{\begin{customblockquote}\list{}{\rightmargin=0em\leftmargin=0em}%
		\item\relax\color{blockquote-text}\ignorespaces}{\unskip\unskip\endlist\end{customblockquote}}

%
% Source Sans Pro as the default font family
% Source Code Pro for monospace text
%
% 'default' option sets the default
% font family to Source Sans Pro, not \sfdefault.
%
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
\usepackage[default]{sourcesanspro}
\usepackage{sourcecodepro}
\else % if not pdftex
\fi

%
% heading color
%
\definecolor{heading-color}{RGB}{40,40,40}
\addtokomafont{section}{\color{heading-color}}
% When using the classes report, scrreprt, book,
% scrbook or memoir, uncomment the following line.
%\addtokomafont{chapter}{\color{heading-color}}

%
% variables for title, author and date
%
\usepackage{titling}
\title{Problem Description\\[1ex]\large[510.607]~Neuere Methoden in der Computerlinguistik}
\author{auth1 auth2}
\date{\today}

%
% tables
%

%
% remove paragraph indentation
%
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines

%
%
% Listings
%
%

%
% header and footer
%
\usepackage[headsepline,footsepline]{scrlayer-scrpage}

\newpairofpagestyles{eisvogel-header-footer}{
	\clearpairofpagestyles
	\ihead*{Problem Description}
	\chead*{}
	\ohead*{\today}
	\ifoot*{auth1 auth2}
	\cfoot*{}
	\ofoot*{\thepage}
	\addtokomafont{pageheadfoot}{\upshape}
}
\pagestyle{eisvogel-header-footer}

%
% Define watermark
%

\begin{document}
		\maketitle
	
	
	{
		\setcounter{tocdepth}{3}
		\tableofcontents
		\newpage
	}
	\section{Motivation}\label{motivation}
	University courses routinely leave students with dozens of slide decks and pages of lecture notes that are difficult to revisit efficiently. Turning this raw material into concise study aids—summaries, flashcards, or quick quizzes—demands substantial time and attention that a student rarely has to spare.
	
	Recent advances in Large Language Models (LLMs) may make it possible to automate much of that manual effort. With the right prompts, an LLM may condense dense transcripts and slide content into digestible bullet points and generate focused practice questions, giving students an immediate way to reinforce key concepts while freeing instructors from repetitive content creation.
	
	By utilizing LLM capabilities for summarization and question generation, our project aims to close the gap between the volume of lecture material produced and the limited time learners have to review it—providing a practical, time-saving tool that supports both comprehension and active recall.	
	
	\section{Problem Description}\label{problem-description}
	Processing the sheer volume of material generated by a single university course is demanding even for experienced humans, and producing high-quality summaries adds another layer of complexity. Prompting a large language model (LLM) to create concise yet faithful summaries is challenging in its own right, but judging the quality of those summaries is harder still. In most courses there are no instructor-approved reference texts against which to compare, and any thorough evaluation requires subject-matter expertise. Complicating matters further are well-known LLM issues—hallucination, omission of crucial details, or oversimplification—that can quietly degrade a summarie's value. For these reasons, lecture summarisation is a compelling but non-trivial test case for LLM technology.

	The next step—transforming those summaries into effective flashcards or quiz questions—poses a different set of hurdles. Structuring an LLM's output so it can be imported into tools such as \emph{Anki} is straightforward; the difficulty lies in ensuring that the questions probe genuine understanding rather than superficial recall. An LLM must identify the lecture’s core concepts, generate questions that reach beyond simple yes--no prompts, and provide answers that are both correct and appropriately matched to each question. Maintaining that alignment while covering the full breadth and depth of the source material is where the true challenge—and the most interesting evaluation opportunities—reside.
	
	\section{Approach}\label{approach}

	Our pipeline is divided into tightly-coupled stages. First, lecture artefacts are normalised and routed to the most suitable language model. Second, carefully crafted prompts instruct each model to produce a Markdown summary followed-in a separate pass-by a set of flashcards. Third, an evaluation loop measures the quality of \emph{both} artefacts, combining low-cost automatic signals with a light layer of expert review.

	\subsection{Prompting}

	The summarizing LLM is given a three-part prompt stack: a \emph{system} message that declares global constraints, a \emph{user} message containing the lecture text, and a single \emph{assistant} message with a worked example.

	...

	\subsection{Evaluation}

	Quality is assessed at two levels—one for the Markdown summary, one for the flashcards—each combining automated checks with targeted human review.

	An automatic script flags hallucinations by verifying that every noun phrase in the summary appears somewhere in the source lecture. Keyword coverage is computed against a short, instructor-supplied list of essential terms; summaries covering < 80 \% trigger re-prompting. Readability is measured with the Flesch-Kincaid grade level (target $\leqslant$ 13). Finally, a teaching assistant inspects a 10\% sample, rating completeness and clarity on a five-point Likert scale; scores below 4 require prompt tuning before release.
	
	\pagebreak
	
	Each card passes through three automatic filters: duplication detection (via similarity hashing), answer-question alignment (via simple natural-language inference), and Bloom-level balance (at least 40 \% cards $\geqslant$ \texttt{APPLY}).  A 10\% human sample is then checked for factual correctness and distractor plausibility. If more than two cards fail, the entire set is regenerated with adjusted prompts.
	
	\section{Expected Results}\label{expected-results}
	
	The project aims to deliver two concrete artefacts for every lecture: a Markdown summary and a corresponding set of flashcards/quiz questions in state-of-the-art flashcard formats. Both products are expected to meet quantifiable quality thresholds while dramatically reducing the manual effort normally required from students.

	\subsection{Summaries}\label{results:summaries}

	We expect each generated summary to satisfy four criteria:

	\begin{enumerate}
	\item \textbf{Fidelity.}  At least 80\,\% of instructor–supplied ``key terms'' should appear in the Markdown, and no hallucinated concepts should be detected by the automatic noun‐phrase cross-check.
	\item \textbf{Clarity.}  Readability should fall at or below a Flesch–Kincaid grade level of~13, making the text accessible to first-year undergraduates.
	\item \textbf{Structural consistency.}  Each summary will contain a top-level title, up to five second-level headings (one per topic) and nested bullet lists that preserve slide identifiers and inline equations. This structure enables quick scanning and precise back-reference to the original slides.
	\item \textbf{Efficiency gains.}  Generating and spot-checking a summary should take \(\leq\) 10 minutes, compared with the 30–45 minutes typically needed for manual drafting, thus saving lecturers or teaching assistants roughly six working days over a 12-lecture course.
	\end{enumerate}
	
	\pagebreak

	\subsection{Flashcards and Quizzes}\label{results:flashcards}

	For the flashcard/quiz component we anticipate the following outcomes:

	\begin{enumerate}
	\item \textbf{Topical coverage.}  The card set should reflect the lecture’s emphasis: at least 90\,\% of the lecture's major topics (as marked by second-level headings in the summary) appear in at least one question.
	\item \textbf{Cognitive depth.}  A minimum of 40\,\% of items are tagged \texttt{APPLY} or higher on Bloom’s taxonomy, ensuring students are challenged beyond rote memorisation.
	\item \textbf{Accuracy and alignment.}  Human spot-checking is expected to confirm \(\geq\) 95\,\% factual correctness and proper question–answer pairing, with fewer than two ambiguous distractors per 100 cards.
	\item \textbf{Seamless integration.}  All cards are emitted in a file format validated for direct import into \emph{Anki}, requiring no manual reformatting and enabling immediate deployment in spaced-repetition workflows.
	\end{enumerate}

	Taken together, these targets define success for the prototype: rapid, reliable generation of study materials that preserve instructional intent, promote active recall, and significantly cut down preparation time for exams.
	
\end{document}
